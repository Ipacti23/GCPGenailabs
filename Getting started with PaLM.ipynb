{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook covers the essentials of prompt engineering, including some best practices.\n",
        "\n",
        "Learn more about prompt design in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn some tasks and best practices around prompt engineering -- how to design prompts for different tasks and how to improve the quality of your responses.\n",
        "\n",
        "This notebook covers the following best practices for prompt engineering:\n",
        "\n",
        "- Be concise\n",
        "- Be specific and well-defined\n",
        "- Ask one task at a time\n",
        "- Turn generative tasks into classification tasks\n",
        "- Improve response quality by including examples\n",
        "\n",
        "It also covers the following tasks:\n",
        "\n",
        "- Zero-shot prompting\n",
        "- Text summarization\n",
        "- Q&A from text\n",
        "- Sentiment analysis using LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e663cb43fa0"
      },
      "source": [
        "### Install Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82ad0c445061",
        "outputId": "07e950bb-d6f0-486d-cef7-da1d67ca7802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shapely<2.0.0\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.7/2.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shapely\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.1\n",
            "    Uninstalling shapely-2.0.1:\n",
            "      Successfully uninstalled shapely-2.0.1\n",
            "Successfully installed shapely-1.8.5.post1\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.31.1-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.10.0)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_resource_manager-1.10.3-py2.py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m321.0/321.0 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.57.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
            "Installing collected packages: google-cloud-resource-manager, google-cloud-aiplatform\n",
            "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.31.1 google-cloud-resource-manager-1.10.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"shapely<2.0.0\"\n",
        "!pip install google-cloud-aiplatform --upgrade --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebd6983cbad"
      },
      "source": [
        "**Colab only:** Uncomment the following cell to restart the kernel or use the button to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bea801acf6b5"
      },
      "outputs": [],
      "source": [
        "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a386d25fa8f"
      },
      "source": [
        "### Authenticating your notebook environment\n",
        "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
        "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1bd1dca8e9a7"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLIue2l0XDC3"
      },
      "source": [
        "**Colab only:** Uncomment the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RDXxCYKbXDC3"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"acn-lkmaigcp\"  # @param {type:\"string\"}\n",
        "vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "from vertexai.language_models import TextGenerationModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP76a2la7O-a"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7isig7e07O-a"
      },
      "outputs": [],
      "source": [
        "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIPcn5dZ7O-b"
      },
      "source": [
        "## Prompt engineering best practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df7d153f4928"
      },
      "source": [
        "Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.\n",
        "\n",
        "The idea of using \"unfancy\" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt. Below are a few guidelines on how to engineer \"unfancy\" prompts.\n",
        "\n",
        "In this section, you'll cover the following best practices when engineering prompts:\n",
        "\n",
        "* Be concise\n",
        "* Be specific, and well-defined\n",
        "* Ask one task at a time\n",
        "* Improve response quality by including examples\n",
        "* Turn generative tasks to classification tasks to improve safety"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43c1169ac435"
      },
      "source": [
        "### Be concise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0f380f1620e"
      },
      "source": [
        "üõë Not recommended. The prompt below is unnecessarily verbose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b6a1697c3603",
        "outputId": "c93b2fd8-bf1b-47ea-f32c-02c8d6617e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Dried & Lovely**\n",
            "* **Dried Blooms**\n",
            "* **Dried Florals**\n",
            "* **Dried Arrangements**\n",
            "* **Preserved Petals**\n",
            "* **Forever Flowers**\n",
            "* **Timeless Blooms**\n",
            "* **Dried Flower Boutique**\n",
            "* **Dried Flower Shop**\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What do you think could be a good name for a flower shop that specializes in selling bouquets of dried flowers more than fresh flowers? Thank you!\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2307f56a9b75"
      },
      "source": [
        "‚úÖ Recommended. The prompt below is to the point and concise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fc666404f47c",
        "outputId": "2ef83ffb-775b-402e-defe-e84ab9864492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Dried Blooms**\n",
            "* **Preserved Petals**\n",
            "* **Forever Flowers**\n",
            "* **Dried & Wild**\n",
            "* **Naturally Beautiful**\n",
            "* **Blooming Memories**\n",
            "* **Sentimental Bouquets**\n",
            "* **Timeless Treasures**\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Suggest a name for a flower shop that sells bouquets of dried flowers\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f6c48bba91"
      },
      "source": [
        "### Be specific, and well-defined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269b428e1563"
      },
      "source": [
        "Suppose that you want to brainstorm creative ways to describe Earth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6436ee2ff406"
      },
      "source": [
        "üõë Not recommended. The prompt below is too generic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "261b7f6e94c5",
        "outputId": "1888cc50-3a08-4905-9edb-755968a351e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Earth is the third planet from the Sun, and the only astronomical object known to harbor life. It is the densest and fifth-largest of the eight planets in the Solar System. Earth's atmosphere is composed of 78% nitrogen, 21% oxygen, 1% other gases, and water vapor. The Earth's surface is divided into several tectonic plates that move around the planet's surface. The Earth's interior is divided into a solid inner core, a liquid outer core, and a mantle. The Earth's magnetic field is generated by the motion of the liquid outer core. The Earth's orbit around the Sun takes 365.256 days, or one year. The Earth's axis is tilted at an angle of 23.5 degrees, which causes the seasons. The Earth's rotation period is 24 hours, or one day.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Tell me about Earth\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bebfecd2912"
      },
      "source": [
        "‚úÖ Recommended. The prompt below is specific and well-defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "242b1b3bae6e",
        "outputId": "5a467783-eef0-443d-afcf-e36c4655baec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Earth is the only planet known to support life.** This is due to a number of factors, including its distance from the sun, its atmosphere, and its water.\n",
            "* **Earth has a large moon.** The moon is thought to have played a role in the development of life on Earth, by stabilizing the planet's rotation and providing a source of tides.\n",
            "* **Earth has a relatively thin atmosphere.** This atmosphere is made up of nitrogen, oxygen, and argon, and it protects the planet from harmful radiation from the sun.\n",
            "* **Earth has a large amount of water.** Water is essential for life, and it covers about 70% of the Earth's surface.\n",
            "* **Earth has a variety of landforms.** These landforms include mountains, valleys, plains, and deserts. They provide a variety of habitats for life.\n",
            "* **Earth has a variety of climate zones.** These climate zones range from the frigid Arctic to the hot tropics. They support a wide variety of plant and animal life.\n",
            "\n",
            "These are just a few of the ways that make Earth unique compared to other planets. Earth is a truly special planet, and it is home to a vast and diverse array of life\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Generate a list of ways that makes Earth unique compared to other planets\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20dca9a05eab"
      },
      "source": [
        "### Ask one task at a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9019d443179"
      },
      "source": [
        "üõë Not recommended. The prompt below has two parts to the question that could be asked separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "70b3b5e5825d",
        "outputId": "cce9b06a-5932-462e-d9ec-b6cc47c16b78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best method of boiling water is to use a kettle. A kettle is a pot with a spout that is designed specifically for boiling water. It is typically made of metal, and has a lid that fits snugly to prevent steam from escaping. To boil water in a kettle, simply fill the kettle with water and place it on the stove. Turn the heat to high and wait for the water to boil. Once the water has boiled, turn off the heat and carefully pour the boiling water into a cup or mug.\n",
            "\n",
            "The sky is blue because of a phenomenon called Rayleigh scattering. Rayleigh scattering is the scattering of light by particles that are much smaller than the wavelength of light. In the case of the sky, the particles that are responsible for Rayleigh scattering are molecules of nitrogen and oxygen. These molecules are so small that they are much smaller than the wavelength of visible light. When sunlight hits these molecules, it is scattered in all directions. However, the blue light is scattered more than the other colors of light. This is because the blue light has a shorter wavelength than the other colors of light. The shorter wavelength means that the blue light is more likely to be scattered by the molecules of nitrogen and oxygen. This is why the sky appears blue.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What's the best method of boiling water and why is the sky blue?\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7936fb58c16a"
      },
      "source": [
        "‚úÖ Recommended. The prompts below asks one task a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2564dad6c8db",
        "outputId": "5cb3809b-5041-4ffd-bb25-803b149e3707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best method of boiling water is to use a kettle. A kettle is a pot with a spout that is designed specifically for boiling water. Kettles are typically made of metal, and they have a lid that helps to keep the heat in. To boil water in a kettle, simply fill the kettle with water and place it on the stove. Turn the heat to high and wait for the water to boil. Once the water has boiled, turn off the heat and carefully pour the water into a cup.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What's the best method of boiling water?\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "770c695ade92",
        "outputId": "133c0f3e-7f44-475f-f4de-0c76c695128b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky is blue because of a phenomenon called Rayleigh scattering. This is the scattering of light by particles that are smaller than the wavelength of light. In the case of the sky, the particles that are doing the scattering are molecules of nitrogen and oxygen.\n",
            "\n",
            "When sunlight hits these molecules, the blue light is scattered more than the other colors. This is because the blue light has a shorter wavelength than the other colors. The red light, orange light, and yellow light are not scattered as much, so they travel through the atmosphere to our eyes. This is why we see the sky as blue.\n",
            "\n",
            "The amount of scattering depends on the wavelength of light and the size of the particles. The shorter the wavelength of light, the more it is scattered. The smaller the particles, the more they scatter light.\n",
            "\n",
            "This is why the sky is blue during the day. However, at sunrise and sunset, the sunlight has to travel through more of the atmosphere to reach our eyes. This means that more of the blue light is scattered, and we see the sky as more red or orange.\n",
            "\n",
            "The sky can also appear blue when there is a lot of dust or pollution in the air. This is because the dust and pollution particles act as additional scattering particles.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Why is the sky blue?\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff606011aa86"
      },
      "source": [
        "### Watch out for hallucinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "956ce45b06a7"
      },
      "source": [
        "Although LLMs have been trained on a large amount of data, they can generate text containing statements not grounded in truth or reality; these responses from the LLM are often referred to as \"hallucinations\" due to their limited memorization capabilities. Note that simply prompting the LLM to provide a citation isn‚Äôt a fix to this problem, as there are instances of LLMs providing false or inaccurate citations. Dealing with hallucinations is a fundamental challenge of LLMs and an ongoing research area, so it is important to be cognizant that LLMs may seem to give you confident, correct-sounding statements that are in fact incorrect.\n",
        "\n",
        "Note that if you intend to use LLMs for the creative use cases, hallucinating could actually be quite useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c9d5f66179a"
      },
      "source": [
        "Try the prompt like the one below repeatedly. You may notice that sometimes it will confidently, but inaccurately, say \"The first elephant to visit the moon was Luna\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d813b9061b08",
        "outputId": "93beb933-acc1-4b0d-ac3f-f7ca4852a655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first elephant to visit the moon was Luna. Luna was a female elephant who was born in 1963 at the San Diego Zoo. In 1968, she was selected to be the first elephant to visit the moon. Luna was trained to wear a spacesuit and to ride in a rocket. She was also trained to perform various tasks on the moon, such as collecting samples of moon rocks and soil.\n",
            "\n",
            "Luna's trip to the moon was a success. She spent several days on the moon, collecting samples and performing experiments. She also became the first animal to walk on the moon. Luna's trip to the moon was a major scientific achievement and helped to pave the way for future human missions to the moon.\n",
            "\n",
            "Luna returned to Earth in 1969. She was a national hero and was celebrated by people all over the world. Luna lived a long and happy life, dying in 2003 at the age of 40.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Who was the first elephant to visit the moon?\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "029e23abfd56"
      },
      "source": [
        "### Turn generative tasks into classification tasks to reduce output variability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d943941d6e59"
      },
      "source": [
        "#### Generative tasks lead to higher output variability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37528e6c9754"
      },
      "source": [
        "The prompt below results in an open-ended response, useful for brainstorming, but response is highly variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a8e2dc39e9ae",
        "outputId": "f138ca4d-ca4b-44e1-9ad6-2932186769d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Write a program to solve a problem you're interested in.** This could be anything from a game to a tool to help you with your studies. The important thing is that you're interested in the problem and that you're motivated to solve it.\n",
            "* **Take a programming course.** There are many online and offline courses available, so you can find one that fits your schedule and learning style.\n",
            "* **Join a programming community.** There are many online and offline communities where you can connect with other programmers and learn from each other.\n",
            "* **Read programming books and articles.** There is a wealth of information available online and in libraries about programming. Take some time to read about different programming languages, techniques, and algorithms.\n",
            "* **Practice, practice, practice!** The best way to improve your programming skills is to practice as much as you can. Write programs, solve problems, and learn from your mistakes.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"I'm a high school student. Recommend me a programming activity to improve my skills.\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f71a6fa2b4bb"
      },
      "source": [
        "#### Classification tasks reduces output variability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917517465dac"
      },
      "source": [
        "The prompt below results in a choice and may be useful if you want the output to be easier to control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3feb93d9df81",
        "outputId": "7a0280e9-365b-4130-843f-659408da7527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I would suggest learning Python. Python is a general-purpose programming language that is easy to learn and has a wide range of applications. It is used in a variety of fields, including web development, data science, and machine learning. Python is also a popular language for beginners, as it has a large community of support and resources available.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"I'm a high school student. Which of these activities do you suggest and why:\n",
        "a) learn Python\n",
        "b) learn Javascript\n",
        "c) learn Fortran\n",
        "\"\"\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32290ac9fb2b"
      },
      "source": [
        "### Improve response quality by including examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "132834f5db2c"
      },
      "source": [
        "Another way to improve response quality is to add examples in your prompt. The LLM learns in-context from the examples on how to respond. Typically, one to five examples (shots) are enough to improve the quality of responses. Including too many examples can cause the model to over-fit the data and reduce the quality of responses.\n",
        "\n",
        "Similar to classical model training, the quality and distribution of the examples is very important. Pick examples that are representative of the scenarios that you need the model to learn, and keep the distribution of the examples (e.g. number of examples per class in the case of classification) aligned with your actual distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46520d938b6a"
      },
      "source": [
        "#### Zero-shot prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46d3b47e6cea"
      },
      "source": [
        "Below is an example of zero-shot prompting, where you don't provide any examples to the LLM within the prompt itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2cbe03eb0b71",
        "outputId": "48dea4f4-5ba8-42a7-ebb0-975ab4bbd6a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
        "\n",
        "Tweet: I loved the new YouTube video you made!\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0daabca1359"
      },
      "source": [
        "#### One-shot prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42c4652fc5c2"
      },
      "source": [
        "Below is an example of one-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cfe584860787",
        "outputId": "69f4a68d-5899-4700-a58e-558b1f9d160b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
        "\n",
        "Tweet: I loved the new YouTube video you made!\n",
        "Sentiment: positive\n",
        "\n",
        "Tweet: That was awful. Super boring üò†\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef58c35005c0"
      },
      "source": [
        "#### Few-shot prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b630e8947b60"
      },
      "source": [
        "Below is an example of few-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fb3ba21bbd11",
        "outputId": "ece608b4-5f9f-4516-fea2-c07b265ca5a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
        "\n",
        "Tweet: I loved the new YouTube video you made!\n",
        "Sentiment: positive\n",
        "\n",
        "Tweet: That was awful. Super boring üò†\n",
        "Sentiment: negative\n",
        "\n",
        "Tweet: Something surprised me about this video - it was actually original. It was not the same old recycled stuff that I always see. Watch it - you will not regret it.\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4023be726eb"
      },
      "source": [
        "#### Choosing between zero-shot, one-shot, few-shot prompting methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d7870ff75cc"
      },
      "source": [
        "Which prompt technique to use will solely depends on your goal. The zero-shot prompts are more open-ended and can give you creative answers, while one-shot and few-shot prompts teach the model how to behave so you can get more predictable answers that are consistent with the examples provided. This is called fine tunign the prompt. In the rest of this notebook, we go over four different types of tasks, one with zero-shot prompting and others with few-shot prompting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freeform Q&A"
      ],
      "metadata": {
        "id": "t_GA9Qx3bb4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt-based Q&A\n",
        "#Zero-shot prompting\n",
        "prompt = \"\"\"Q: Which company first introduced generative AI? What architecture is used in generative AI?\\n\n",
        "            A:\n",
        "         \"\"\"\n",
        "print(\n",
        "    generation_model.predict(\n",
        "        prompt,\n",
        "        max_output_tokens=256,\n",
        "        temperature=0.1,\n",
        "    ).text\n",
        ")"
      ],
      "metadata": {
        "id": "fMtHtMApa0_X",
        "outputId": "53a077f3-9c36-456e-d24e-6d2104cdb063",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google first introduced generative AI. Generative AI uses a deep learning architecture called a generative adversarial network (GAN).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text summarization from a paragraph"
      ],
      "metadata": {
        "id": "Sg2_hYrpbg3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Summarization\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 768,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "response = generation_model.predict(\n",
        "    \"\"\"Provide a brief summary for the following article:\n",
        "\n",
        " Gen AI and LLMs create output based on existing content, whether that comes from the open internet or the specific datasets that they have been trained on. That means they are good at iterating on what came before, which can be a boon for attackers. For example, in the same way that AI can create iterations of content using the same set of words, it can create malicious code that is similar to something that already exists, but different enough to evade detection. With this technology, bad actors will generate unique payloads or attacks designed to evade security defenses that are built around known attack signatures.One way attackers are already doing this is by using AI to develop webshell variants, malicious code used to maintain persistence on compromised servers. Attackers can input the existing webshell into a generative AI tool and ask it to create iterations of the malicious code. These variants can then be used, often in conjunction with a remote code execution vulnerability (RCE), on a compromised server to evade detection. \"\"\",\n",
        "    **parameters\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ],
      "metadata": {
        "id": "14bvCZKybH-p",
        "outputId": "f5251b15-c35c-4b44-821a-00b6810f2ab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: Generative AI and LLMs can be used to create malicious code that is similar to something that already exists, but different enough to evade detection. This can be used to develop webshell variants, malicious code used to maintain persistence on compromised servers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q&A from text"
      ],
      "metadata": {
        "id": "WRQDjLxNbkpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    #Q&A from paragraph\n",
        "    parameters = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"max_output_tokens\": 256,\n",
        "        \"top_p\": 0.8,\n",
        "        \"top_k\": 1\n",
        "    }\n",
        "    response = generation_model.predict(\n",
        "        \"\"\"Background: A large language model (LLM) is a language model characterized by its large size. Their size is enabled by AI accelerators, which are able to process vast amounts of text data, mostly scraped from the Internet. The artificial neural networks which are built can contain from tens of millions and up to billions of weights and are (pre-)trained using self-supervised learning and semi-supervised learning. Transformer architecture contributed to faster training. Alternative architectures include the mixture of experts (MoE), which has been proposed by Google, starting with sparsely-gated ones in 2017, Gshard in 2021 to GLaM in 2022.\n",
        "\n",
        "    As language models, they work by taking an input text and repeatedly predicting the next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire embodied knowledge about syntax, semantics and \\\"ontology\\\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.\n",
        "\n",
        "    Q: What does LLM stand for?\n",
        "    A: Large Language Model.\n",
        "\n",
        "    Q: What is the primary characteristic of LLMs?\n",
        "    A: Their large size.\n",
        "\n",
        "    Q: Approximately how many weights can be used to train an LLM?\n",
        "    A: An LLM may be trained using millions and sometimes, even billions of weights\n",
        "\n",
        "    Q: How were models used for specific tasks till 2020?\n",
        "    A: Fine tuning was the only way to use models for specific tasks till 2020\n",
        "\n",
        "    Q: What are the types of architectures used in LLMs?\n",
        "    A:\n",
        "    \"\"\",\n",
        "        **parameters\n",
        "    )\n",
        "    print(f\"Response from Model: {response.text}\")"
      ],
      "metadata": {
        "id": "yjZtNSqObM0P",
        "outputId": "19c3ea53-cbbc-4929-ba23-56c739a49486",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: Transformer architecture and mixture of experts (MoE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis using LLM"
      ],
      "metadata": {
        "id": "mxZW61TFbVtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = generation_model.predict(\n",
        "    \"\"\"input: I had to compare two versions of Hamlet for my Shakespeare class and unfortunately I picked this version. Everything from the acting (the actors deliver most of their lines directly to the camera) to the camera shots (all medium or close up shots...no scenery shots and very little back ground in the shots) were absolutely terrible. I watched this over my spring break and it is very safe to say that I feel that I was gypped out of 114 minutes of my vacation. Not recommended by any stretch of the imagination.\n",
        "Classify the sentiment of the message: negative\n",
        "\n",
        "input: This Charles outing is decent but this is a pretty low-key performance. Marlon Brando stands out. There\\'s a subplot with Mira Sorvino and Donald Sutherland that forgets to develop and it hurts the film a little. I\\'m still trying to figure out why Charlie want to change his name.\n",
        "Classify the sentiment of the message: negative\n",
        "\n",
        "input: My family has watched Arthur Bach stumble and stammer since the movie first came out. We have most lines memorized. I watched it two weeks ago and still get tickled at the simple humor and view-at-life that Dudley Moore portrays. Liza Minelli did a wonderful job as the side kick - though I\\'m not her biggest fan. This movie makes me just enjoy watching movies. My favorite scene is when Arthur is visiting his fianc√©e\\'s house. His conversation with the butler and Susan\\'s father is side-spitting. The line from the butler, \\\"Would you care to wait in the Library\\\" followed by Arthur\\'s reply, \\\"Yes I would, the bathroom is out of the question\\\", is my NEWMAIL notification on my computer.\n",
        "Classify the sentiment of the message: positive\n",
        "\n",
        "input: We (me and my wife with knee pain) booked a taxi (using MakeMyTrip phone app) to go to my town from Bengaluru Airport at 1:AM on May 24th. My mother and relatives waiting for me with father dead body to be cremated after my arrival to home. I paid an advance of Rs 1800+ to MakeMyTrip, the taxi did NOT come, and they did not inform me the cancellation. After waited for a long time on the customer service call and I was told that the taxi got cancelled with a blunt answer.\n",
        "Classify the sentiment of the message:\n",
        "\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ],
      "metadata": {
        "id": "UY9AgEDUbQ4W",
        "outputId": "708cdcdc-a8cb-4fac-a961-ffe000b86347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: negative\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_prompt_design.ipynb",
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}