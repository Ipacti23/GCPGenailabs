{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d00c0f8-c0f2-40f5-b662-dde8a9aef8f7",
   "metadata": {},
   "source": [
    "Semantic search is a type of search that uses the meaning of words and phrases to find relevant results.\n",
    "\n",
    "In this tutorial, we will demonstrate how to do semantic search with embeddings generated from the news text (taken from a sample dataset in Google Cloud) and using Google ScaNN: Efficient Vector Similarity Search to retrieve the most relevant news semantically.The generated embeddings form a vector space which is then compared to a similar vector space of the news data to do a vector similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb008181-d747-4b09-b28d-b3e355381625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Vertex AI Python SDK\n",
    "!pip3 install google-cloud-aiplatform>=1.25 \"shapely<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7b356-dfbd-4cd9-a3e8-dea6aaf91c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize SDK\n",
    "PROJECT_ID = \"acn-lkmaigcp\"\n",
    "LOCATION = \"us-central1\" \n",
    "\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190e41b-ef82-4497-9590-fa2fdbeb1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Text Embedding Model\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a74a9-8119-4750-b705-1cfd0c0fb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install ScaNN Package\n",
    "!pip3 install scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e500e9e-2ff2-41d1-8b81-cd7e4ce4b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install other required packages\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scann\n",
    "\n",
    "from typing import Union\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b37de1-9310-44a9-bb29-972e52f5f28d",
   "metadata": {},
   "source": [
    "Getting Stackoverflow data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9aaaca-4f7a-463c-80a0-6696d5c00541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "    Args:\n",
    "        sql: SQL query, as a string, to execute in BigQuery\n",
    "    Returns:\n",
    "        df: DataFrame of results from query,  or error, if any\n",
    "    \"\"\"\n",
    "\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    df = client_result.result().to_arrow().to_pandas()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b01bd6-d1a9-4425-a225-366fe0d25baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the query\n",
    "records = run_bq_query(\n",
    "    \"\"\"SELECT\n",
    "    CONCAT(q.title, q.body) as input_text,\n",
    "    a.body AS output_text\n",
    "FROM\n",
    "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "JOIN\n",
    "    `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "ON\n",
    "    q.accepted_answer_id = a.id\n",
    "WHERE\n",
    "    q.accepted_answer_id IS NOT NULL AND\n",
    "    REGEXP_CONTAINS(q.tags, \"python\") AND\n",
    "    a.creation_date >= \"2020-01-01\"\n",
    "LIMIT\n",
    "    100\n",
    "\"\"\"\n",
    ")\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98850d2-5d85-4f20-b31c-c47add961984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek at the data.\n",
    "df = pd.DataFrame(records)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62f31b-22e2-4934-b7ca-b88129229cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Embeddings\n",
    "def get_embedding(text):\n",
    "    get_embedding.counter += 1\n",
    "    try:\n",
    "        if get_embedding.counter % 100 == 0:\n",
    "            time.sleep(3)\n",
    "        return model.get_embeddings([text])[0].values\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "get_embedding.counter = 0\n",
    "\n",
    "# This may take several minutes to complete.\n",
    "df[\"embedding\"] = df[\"input_text\"].apply(lambda x: get_embedding(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f332589-483f-4484-8c50-16ee82919823",
   "metadata": {},
   "source": [
    "Create an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b664c-abe6-4c44-8983-0acb53ff5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_count = len(records)\n",
    "dataset = np.empty((record_count, 768))\n",
    "for i in range(record_count):\n",
    "    dataset[i] = df.embedding[i]\n",
    "\n",
    "normalized_dataset = dataset / np.linalg.norm(dataset, axis=1)[:, np.newaxis]\n",
    "# configure ScaNN as a tree - asymmetric hash hybrid with reordering\n",
    "# anisotropic quantization as described in the paper; see README\n",
    "\n",
    "# use scann.scann_ops.build() to instead create a TensorFlow-compatible searcher\n",
    "searcher = (\n",
    "    scann.scann_ops_pybind.builder(normalized_dataset, 10, \"dot_product\")\n",
    "    .tree(\n",
    "        num_leaves=record_count,\n",
    "        num_leaves_to_search=record_count,\n",
    "        training_sample_size=record_count,\n",
    "    )\n",
    "    .score_ah(2, anisotropic_quantization_threshold=0.2)\n",
    "    .reorder(100)\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddaf425-a6b7-40a7-8b31-1edc85f031b2",
   "metadata": {},
   "source": [
    "Query the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b354016-99e0-4e3b-b7df-8707bdba6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_count = len(records)\n",
    "dataset = np.empty((record_count, 768))\n",
    "for i in range(record_count):\n",
    "    dataset[i] = df.embedding[i]\n",
    "\n",
    "normalized_dataset = dataset / np.linalg.norm(dataset, axis=1)[:, np.newaxis]\n",
    "# configure ScaNN as a tree - asymmetric hash hybrid with reordering\n",
    "# anisotropic quantization as described in the paper; see README\n",
    "\n",
    "# use scann.scann_ops.build() to instead create a TensorFlow-compatible searcher\n",
    "searcher = (\n",
    "    scann.scann_ops_pybind.builder(normalized_dataset, 10, \"dot_product\")\n",
    "    .tree(\n",
    "        num_leaves=record_count,\n",
    "        num_leaves_to_search=record_count,\n",
    "        training_sample_size=record_count,\n",
    "    )\n",
    "    .score_ah(2, anisotropic_quantization_threshold=0.2)\n",
    "    .reorder(100)\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167187de-fbac-44c9-8516-c82282b22536",
   "metadata": {},
   "source": [
    "Query the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41d7e2-1702-4394-b0de-85d7824f12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    start = time.time()\n",
    "    query = model.get_embeddings([query])[0].values\n",
    "    neighbors, distances = searcher.search(query, final_num_neighbors=3)\n",
    "    end = time.time()\n",
    "\n",
    "    for id, dist in zip(neighbors, distances):\n",
    "        print(f\"[docid:{id}] [{dist}] -- {df.input_text[int(id)][:125]}...\")\n",
    "    print(\"Latency (ms):\", 1000 * (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e9e36-e3dc-48d6-b55d-e62d97a8a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"How can I convert videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21191a96-371a-4eb4-9090-6f3894c54030",
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"tell me about Pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6694809-e92d-4ad6-bec7-37be999da276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
